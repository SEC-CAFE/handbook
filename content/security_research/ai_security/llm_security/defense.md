---
title: 安全加固与防御
weight: 3
description: 安全加固与防御
keywords:
 - LLM Security
 - AI BOM
 - LangChain 安全
 - AI数据安全
 - awesome llm security
 - AI攻击
 - AI测试框架
 - AI测试方法
 - AI攻击实例
 - 安全测试基准
 - 测试基线
 - AI安全防御
---


## 安全加固与防御
- [大语言模型安全指南：如何提升LLM安全抵御Prompt Injection](https://www.freebuf.com/articles/security-management/393637.html)
- [代码安全](https://www.freebuf.com/articles/neopoints/384798.html)
- [追问新知｜大语言模型如何提升自我防御技能？](https://mp.weixin.qq.com/s/Mv9bP48n7RZWxz8x_SJuLQ)
- [AI&机器学习的威胁建模](https://mp.weixin.qq.com/s/w6yUHXE97Hov_i87JoYudQ)
- [【AI安全】终章-构建可用的人工智能风险管理框架](https://mp.weixin.qq.com/s/Uh7vhKo6P-rs6dEU0pBaLw)
- [Google's Secure AI Framework](https://saif.google/)
- [Protecting Large Language Models](https://bughunters.google.com/blog/5679863572070400/protecting-large-language-models)

## AI攻击检测
- [威胁狩猎新挑战——LLM时代如何解码黑客攻击](https://www.freebuf.com/articles/network/373156.html)

## 企业实践与分享
- [vivo对生成式人工智能的安全思考和实践](#)
- [OpenAI安全系统负责人长文梳理：大模型的对抗攻击与防御](https://mp.weixin.qq.com/s/t87IOi6r4N-c-StI9CPy_A)
- [Meta开源大模型的安全实践](https://mp.weixin.qq.com/s/-mHiEfImfZBgotDgRnKU5w)
- [为什么我们需要 Hugging Face 的 Safetensors？](https://mp.weixin.qq.com/s/o-banwxQlk4rBiNOnp6lmA)

## 工具
- [Garak](https://docs.garak.ai/garak/) 一个用于大语言模型 (LLM) 漏洞扫描的 Python 包
- [PyRIT](https://github.com/Azure/PyRIT) 微软发布的用于生成式人工智能的Python风险识别工具包
- [GPTFUZZER](https://github.com/sherdencooper/GPTFuzz) 利用自动生成的越狱提示对大型语言模型进行红队测试
- [PromptBench](https://github.com/microsoft/promptbench)  一个用于测量LLMs对对抗性提示的鲁棒性的基准测试
- [securityGPT](https://github.com/GPTGeeker/securityGPT) 面向GPT开发者的安全Prompt组件
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) Meta出品的一套评估和改进LLM安全性的工具
- [Dioptra](https://github.com/usnistgov/dioptra) NIST发布的AI模型风险测试平台，用于评估AI风险和恶意攻击
- [Agent Security Bench (ASB)](https://github.com/agiresearch/ASB) 一个综合框架，旨在形式化、基准化和评估基于LLM agent的攻击和防御
